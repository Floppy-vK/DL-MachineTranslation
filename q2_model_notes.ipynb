{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 notes\n",
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lecture 6\n",
    "##### Typical CNN Application\n",
    " \n",
    " Building blocks:\n",
    " 1. Convolution layer\n",
    " 2. Activation layer (typically: ReLu)\n",
    " 3. Pooling or downsampling\n",
    " 4. Fully connected layer (mlp)\n",
    "\n",
    " Example order: \n",
    " - Convolution +Relu\n",
    " - Pooling\n",
    " - Convolution +Relu\n",
    " - Fully Connected\n",
    " - Fully Connected\n",
    " - output\n",
    "\n",
    " notebook example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Create a sequential model\n",
    "model = nn.Sequential()\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "#notebook used Conv2d and MaxPool2d but our data is 1D \n",
    "model.add_module('Conv_1', nn.Conv1d(in_channels=1, out_channels=32, kernel_size=(3,3)))\n",
    "model.add_module('Relu_1', nn.ReLU())\n",
    "model.add_module('MaxPool_1', nn.MaxPool1d(kernel_size=2))\n",
    "\n",
    "model.add_module('Conv_2', nn.Conv1d(in_channels=32, out_channels=64, kernel_size=(3,3)))\n",
    "model.add_module('Relu_2', nn.ReLU())\n",
    "model.add_module('MaxPool_2', nn.MaxPool1d(kernel_size=2))\n",
    "\n",
    "model.add_module('Conv_3', nn.Conv1d(in_channels=64, out_channels=64, kernel_size=(3,3)))\n",
    "model.add_module('Relu_3', nn.ReLU())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other layer options:\n",
    "- Dropout layer (DO)\n",
    "    - safeguards against overfitting\n",
    "- Flatten layer\n",
    "    - decreases the dimensions (we shouldn't need this unless we use MEL)\n",
    "- Dense layer\n",
    "    - a fully connected (FC) layer (think mlp)\n",
    "- Batch normalization (BN)\n",
    "    - helps decrease the number of epochs needed to train model\n",
    "___\n",
    "model architecture from: \n",
    "https://pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/\n",
    "\n",
    "INPUT => [CONV => RELU => BN => CONV => RELU => BN => POOL] * 3 => [FC RELU => BN] * 2 => FC\n",
    "\n",
    "There's also smth called softmax but we shouldn't apply any BN before softmax:\n",
    "\n",
    "\"You do not apply batch normalization before the softmax classifier as at this point we assume our network has learned its discriminative features earlier in the architecture.\"\n",
    "\n",
    "\"The SOFTMAX activation layer is often omitted from the network diagram as it is assumed it directly follows the final FC\"\n",
    "\n",
    "___\n",
    "\n",
    "I recommend we add in some DO layers like so:\n",
    "\n",
    "**     INPUT => [CONV => RELU => BN => CONV => RELU => BN => POOL] * 3 => DO => [FC RELU => BN] * 2 => DO => FC\n",
    "    **\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "Alternatively, the above link also has an \"AlexNet-like\" architecture:\n",
    "\n",
    "INPUT => [CONV => RELU => POOL] * 2 => [CONV => RELU] * 3 => POOL =>\n",
    "\t[FC => RELU => DO] * 2 => SOFTMAX\n",
    "\n",
    "\n",
    "The key features are the repeating conv, relu, and pool in different patterns. This one does not have BN, however, so we can add some in and ?get rid of softmax?\n",
    "\n",
    "**      INPUT => [CONV => RELU => BN => POOL] * 2 => [CONV => RELU] * 3 => BN => POOL =>\n",
    "\t[FC => RELU => DO] * 2 => FC\n",
    "    **    \n",
    "\n",
    "or smth like that.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
