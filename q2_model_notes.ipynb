{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 notes\n",
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lecture 6\n",
    "##### Typical CNN Application\n",
    " \n",
    " Building blocks:\n",
    " 1. Convolution layer\n",
    " 2. Activation layer (typically: ReLu)\n",
    " 3. Pooling or downsampling\n",
    " 4. Fully connected layer (mlp)\n",
    "\n",
    " Example order: \n",
    " - Convolution +Relu\n",
    " - Pooling\n",
    " - Convolution +Relu\n",
    " - Fully Connected\n",
    " - Fully Connected\n",
    " - output\n",
    "\n",
    " notebook example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Create a sequential model\n",
    "model = nn.Sequential()\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "#notebook used Conv2d and MaxPool2d but our data is 1D \n",
    "model.add_module('Conv_1', nn.Conv1d(in_channels=1, out_channels=32, kernel_size=(3,3)))\n",
    "model.add_module('Relu_1', nn.ReLU())\n",
    "model.add_module('MaxPool_1', nn.MaxPool1d(kernel_size=2))\n",
    "\n",
    "model.add_module('Conv_2', nn.Conv1d(in_channels=32, out_channels=64, kernel_size=(3,3)))\n",
    "model.add_module('Relu_2', nn.ReLU())\n",
    "model.add_module('MaxPool_2', nn.MaxPool1d(kernel_size=2))\n",
    "\n",
    "model.add_module('Conv_3', nn.Conv1d(in_channels=64, out_channels=64, kernel_size=(3,3)))\n",
    "model.add_module('Relu_3', nn.ReLU())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other layer options:\n",
    "- Dropout layer (DO)\n",
    "    - safeguards against overfitting\n",
    "- Flatten layer\n",
    "    - decreases the dimensions (we shouldn't need this unless we use MEL)\n",
    "- Dense layer (Linear Layer)\n",
    "    - a fully connected (FC) layer (think mlp)\n",
    "- Batch normalization (BN)\n",
    "    - helps decrease the number of epochs needed to train model\n",
    "___\n",
    "model architecture from: \n",
    "https://pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/\n",
    "\n",
    "INPUT => [CONV => RELU => BN => CONV => RELU => BN => POOL] * 3 => [FC RELU => BN] * 2 => FC\n",
    "\n",
    "There's also smth called softmax but we shouldn't apply any BN before softmax:\n",
    "\n",
    "\"You do not apply batch normalization before the softmax classifier as at this point we assume our network has learned its discriminative features earlier in the architecture.\"\n",
    "\n",
    "\"The SOFTMAX activation layer is often omitted from the network diagram as it is assumed it directly follows the final FC\"\n",
    "\n",
    "___\n",
    "\n",
    "I recommend we add in some DO layers like so:\n",
    "\n",
    "**     INPUT => [CONV => RELU => BN => CONV => RELU => BN => POOL] * 3 => DO => [FC RELU => BN] * 2 => DO => FC\n",
    "    **\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "Alternatively, the above link also has an \"AlexNet-like\" architecture:\n",
    "\n",
    "INPUT => [CONV => RELU => POOL] * 2 => [CONV => RELU] * 3 => POOL =>\n",
    "\t[FC => RELU => DO] * 2 => SOFTMAX\n",
    "\n",
    "\n",
    "The key features are the repeating conv, relu, and pool in different patterns. This one does not have BN, however, so we can add some in and ?get rid of softmax?\n",
    "\n",
    "**      INPUT => [CONV => RELU => BN => POOL] * 2 => [CONV => RELU] * 3 => BN => POOL =>\n",
    "\t[FC => RELU => DO] * 2 => FC\n",
    "    **    \n",
    "\n",
    "or smth like that.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "RNNs are perfect for evaluating audio signals since they are specifically made to work with sequential data + they can capture the temporal dependencies of the audio data. \n",
    "\n",
    "**What type?**\n",
    "\n",
    "Both LSTM and GRU would be suitable for our problem since they're both able to handle long-term dependencies in sequential data. However, if we want to ensure that we're not estimating too many parameters (like they suggest in the assignment), GRU would be more suitable since it uses less parameters than LSTM. Both networks have gate units that regulate the flow of information. \n",
    "\n",
    "**Main differences between LSTM and GRU:**\n",
    "\n",
    "\"These two units however have a number of differences as well. One feature of the LSTM unit that is missing from the GRU is the controlled exposure of the memory content. In the LSTM unit, the amount of the memory content that is seen, or used by other units in the network is controlled by the output gate. On the other hand the GRU exposes its full content without any control.\n",
    "Another difference is in the location of the input gate, or the corresponding reset gate. The LSTM unit computes the new memory content without any separate control of the amount of information flowing from the previous time step. Rather, the LSTM unit controls the amount of the new memory content being added to the memory cell independently from the forget gate. On the other hand, the GRU controls the information flow from the previous activation when computing the new, candidate activation, but does not independently control the amount of the candidate activation being added (the control is tied via the update gate).\" https://arxiv.org/pdf/1412.3555v1.pdf\n",
    "\n",
    "\"GRU uses less training parameter and therefore uses less memory and executes faster than LSTM whereas LSTM is more accurate on a larger dataset. One can choose LSTM if you are dealing with large sequences and accuracy is concerned, GRU is used when you have less memory consumption and want faster results.\" https://analyticsindiamag.com/lstm-vs-gru-in-recurrent-neural-network-a-comparative-study/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
